{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vargan_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRfLtuJvK4Fm"
      },
      "source": [
        "# Using GANS to Estimate Value-at-risk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7yDv_iH7Fau"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE8Q5LWhOKnq"
      },
      "source": [
        "%%capture\n",
        "# Installing Yfinance package used to download data\n",
        "!pip install yfinance --upgrade --no-cache-dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLQ__iwnLLLr"
      },
      "source": [
        "%%capture\n",
        "import tensorflow as tf\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1O0fLhrli3K"
      },
      "source": [
        "## 2. Loading Data from Yahoo finance and Data preparation Steps\n",
        "\n",
        "Loading stock price data used to form the portfolio we estimate VaR for.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYDXT55_LRhj"
      },
      "source": [
        "data_train = yf.download(\"HM-B.ST EKTA-B.ST TEL2-B.ST SEB-A.ST INVE-B.ST\", start=\"2000-02-27\", end=\"2009-12-20\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqcyXohHoFyG"
      },
      "source": [
        "data_train = data_train['Adj Close']\n",
        "data_train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hn2CRuKFwqt"
      },
      "source": [
        "# Impute with previous valid value if there is missing data \n",
        "data_train=data_train.fillna(method='ffill')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aigjqnh3M9bD"
      },
      "source": [
        "# Make into numpy array\n",
        "data_train = data_train.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu_QKEWGTHcP"
      },
      "source": [
        "# Calculate returns\n",
        "def get_returns(data):\n",
        "  return_data = np.empty([data.shape[0]-1, data.shape[1]])\n",
        "  for i in range(data.shape[0]-1):\n",
        "    for a in range(data.shape[1]):\n",
        "      stock_return = ((data[i+1,a]-data[i,a])/data[i,a])\n",
        "      return_data[i,a] = stock_return\n",
        "  return return_data  \n",
        "\n",
        "return_data = get_returns(data_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xV5ld9oSwEA"
      },
      "source": [
        "# Prepare data in correct shape, depends on the forward horizon f of the model\n",
        "def to_stock_M(org_array, horizon):\n",
        "  days = len(org_array)\n",
        "  nbr_matrices = days-horizon+1\n",
        "  stock_M = np.empty((nbr_matrices,horizon,5))\n",
        "  for i in range(nbr_matrices):\n",
        "    end_day = i + horizon \n",
        "    sub_array = org_array[i:end_day,:]\n",
        "    stock_M[i,:,:] = sub_array\n",
        "\n",
        "  return stock_M  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVM6eEAnOP68"
      },
      "source": [
        "# Setting parameters for the model\n",
        "# f: the forwards window for the model\n",
        "# k: number of assets in the portfolio\n",
        "# noise_dim: dimension of input noise for the generator\n",
        "k = 5\n",
        "f = 1\n",
        "BATCH_SIZE = 36\n",
        "noise_dim = 2*k\n",
        "epochs = 10\n",
        "training_data = to_stock_M(return_data, 1).astype(\"float32\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRfycSSiXEoU"
      },
      "source": [
        "# The normalize function is used during the training to normalize the training data\n",
        "\n",
        "# Parameter values for normalization\n",
        "max_tot = np.max(return_data, axis = 0)\n",
        "min_tot = np.min(return_data, axis = 0)\n",
        "mean_tot = np.mean(return_data, axis = 0)\n",
        "std_tot = np.std(return_data, axis = 0)\n",
        "\n",
        "def normalize(data):\n",
        "    norm_data = ((data -mean_tot)/std_tot)   \n",
        "    return norm_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MADuAPQvMjD0"
      },
      "source": [
        "## 3. The Generator\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cvVxd6tDvGE"
      },
      "source": [
        "# Function defining generator \n",
        "def Generator_model():\n",
        "  latent_input = keras.Input(shape=(noise_dim,), name=\"latent\")\n",
        "  x = layers.Dense(32*k, activation=\"relu\")(latent_input)\n",
        "  x=layers.BatchNormalization()(x)\n",
        "  x = layers.Reshape((32,k))(x)\n",
        "  x = layers.Conv1D(filters=k*2, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "  x=layers.BatchNormalization()(x)\n",
        "  x = layers.Conv1D(filters=k*2, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "  x=layers.BatchNormalization()(x)\n",
        "  x = layers.Conv1D(filters=k*2, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "  x=layers.BatchNormalization()(x)\n",
        "  x = layers.Conv1D(filters=k*2, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "  x=layers.BatchNormalization()(x) \n",
        "  outputs = layers.Conv1D(filters=k, kernel_size=5, strides=2, padding=\"same\")(x)\n",
        "  model = keras.Model(inputs=[latent_input], \n",
        "                      outputs=outputs, name=\"Generator_model\")\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Firxfk6hFKMZ"
      },
      "source": [
        "generator = Generator_model()\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldr_h1zHp1MF"
      },
      "source": [
        "# Testing untrained model\n",
        "# Input noise \n",
        "noise=np.random.normal(size=(noise_dim))\n",
        "noise=noise.reshape(1,noise_dim)\n",
        "generated_returns = generator.predict([noise])\n",
        "print(f'Generated returns are: \\n \\n {generated_returns}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FRJVsO2UeqZ"
      },
      "source": [
        "## 4. The Discriminator "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVtrAbgRUmtZ"
      },
      "source": [
        "# Function defining discriminator model\n",
        "def discriminator_model():\n",
        "  M_input = keras.Input(shape=(1,k))\n",
        "  x = layers.Dense(k*20)(M_input)\n",
        "  x = layers.LeakyReLU()(x)\n",
        "  x = layers.Dense(k*10)(x)\n",
        "  x = layers.LeakyReLU()(x)\n",
        "  x = layers.Dense(k*5)(x)\n",
        "  x = layers.LeakyReLU()(x)\n",
        "  output = layers.Dense(1)(x) \n",
        "  model = keras.Model(inputs=[M_input], \n",
        "                      outputs=output, name=\"Discriminator_model\")\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fvrFYSFECT7"
      },
      "source": [
        "discriminator = discriminator_model()\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN4-4EP_34pc"
      },
      "source": [
        "# Testing untrained descriminator discriminator\n",
        "critic_val = discriminator.predict([generated_returns])\n",
        "print(f'Critic value is: \\n \\n {critic_val}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVt-oZoJRROB"
      },
      "source": [
        "# Customize the Training Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9cd77HKRXu9"
      },
      "source": [
        "# Defining a class WGAN that is a subclass of the superclass keras.Model, were we override the train_step\n",
        "\n",
        "# The code used below are based on the Keras code example by A_K_Nain, see below link\n",
        "# https://keras.io/examples/generative/wgan_gp/\n",
        "\n",
        "class WGAN(keras.Model):\n",
        "  def __init__(\n",
        "      self,\n",
        "      discriminator,\n",
        "      generator,\n",
        "      latent_dim,\n",
        "      disc_extra_steps=3,\n",
        "      gp_weight=10.0,\n",
        "\n",
        "  ):\n",
        "      # Initialize superclass\n",
        "      super(WGAN, self).__init__()\n",
        "      # Define attributes  \n",
        "      self.discriminator = discriminator \n",
        "      self.generator = generator \n",
        "      self.latent_dim = latent_dim\n",
        "      self.d_steps = disc_extra_steps\n",
        "      self.gp_weight = gp_weight\n",
        "  # Override the compile method, so it can take additional params     \n",
        "  def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
        "      super(WGAN, self).compile()\n",
        "      self.d_optimizer = d_optimizer \n",
        "      self.g_optimizer = g_optimizer\n",
        "      self.d_loss_fn = d_loss_fn\n",
        "      self.g_loss_fn = g_loss_fn\n",
        "  # Gradient penalty method to get GP that will be added to discriminator loss\n",
        "  def gradient_penalty(self, batch_size, real_data, fake_data):\n",
        "    alpha = tf.random.normal([batch_size, 1, 1], 0.0, 1.0)\n",
        "    diff = fake_data - real_data\n",
        "    interpolated = real_data + alpha * diff # gives weighted sample \n",
        "    with tf.GradientTape() as gp_tape:\n",
        "      gp_tape.watch(interpolated)\n",
        "      # Discriminator output for interpolated matrices  \n",
        "      pred = self.discriminator([interpolated], training=True)\n",
        "      # Gradients with respect to interpolated data\n",
        "      grads = gp_tape.gradient(pred, [interpolated])[0] # ad A aswell?\n",
        "      # Norm of the gradients\n",
        "      norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2])) # some problem here?\n",
        "      gp = tf.reduce_mean((norm-1.0)**2)\n",
        "      return gp\n",
        "  # Now we override the train_step / make a custom training step     \n",
        "  def train_step(self, real_data):\n",
        "    if isinstance(real_data, tuple):\n",
        "      real_data = real_data[0]\n",
        "    batch_size = tf.shape(real_data)[0]\n",
        "    # First the discriminator is trained for a number of steps before discriminator trained a step\n",
        "    real_data = normalize(real_data) \n",
        "\n",
        "    for i in range(self.d_steps):\n",
        "      # Get latent vectors for each batch\n",
        "      random_latent_vectors = tf.random.normal(\n",
        "          shape=(batch_size, self.latent_dim)\n",
        "      )\n",
        "      with tf.GradientTape() as tape:\n",
        "        # Generate fake_data from generator\n",
        "        fake_data = self.generator([random_latent_vectors], training=True)\n",
        "        # Get critic value for fake \n",
        "        fake_logits = self.discriminator([fake_data], training = True)\n",
        "        # Get critic value for real data \n",
        "        real_logits =self.discriminator([real_data], training = True)\n",
        "\n",
        "        # Discriminator loss (wasserstein)\n",
        "        d_cost = self.d_loss_fn(real_dat=real_logits, fake_dat=fake_logits)\n",
        "        # Calculate GP\n",
        "        gp = self.gradient_penalty(batch_size, real_data, fake_data)\n",
        "        # Add Gp to get the total discriminator loss\n",
        "        d_loss = d_cost + gp * self.gp_weight\n",
        "\n",
        "      # Get gradients \n",
        "      d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "      # Uppdating weights\n",
        "      self.d_optimizer.apply_gradients(\n",
        "          zip(d_gradient, self.discriminator.trainable_variables)\n",
        "      )\n",
        "\n",
        "    # Now we train the generator\n",
        "    # Get latent vectors\n",
        "    random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Generate fake data \n",
        "      generated_data = self.generator([random_latent_vectors], training=True)\n",
        "      # Critic value \n",
        "      gen_dat_logits = self.discriminator([generated_data], training=True)\n",
        "      # Calculate loss (wasserstein)\n",
        "      g_loss = self.g_loss_fn(gen_dat_logits)\n",
        "\n",
        "    # Get gradients\n",
        "    gen_gradients = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "    # Update weights of generator \n",
        "    self.g_optimizer.apply_gradients(\n",
        "        zip(gen_gradients, self.generator.trainable_variables)\n",
        "    )\n",
        "    return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9TjviVF5ARY"
      },
      "source": [
        "# Training of Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAxoyMHh4_lo"
      },
      "source": [
        "# Optimizers to use\n",
        "generator_optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=0.00002, beta_1=0.5, beta_2=0.9\n",
        ")\n",
        "\n",
        "discriminator_optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=0.00002, beta_1=0.5, beta_2=0.9\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHmN9sdd5z2w"
      },
      "source": [
        "# Loss functions\n",
        "\n",
        "def discriminator_loss(real_dat, fake_dat):\n",
        "  real_loss = tf.reduce_mean(real_dat)\n",
        "  fake_loss = tf.reduce_mean(fake_dat)\n",
        "  return fake_loss - real_loss\n",
        "\n",
        "def generator_loss(fake_dat):\n",
        "  return -tf.reduce_mean(fake_dat)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ul6-XhM69N-"
      },
      "source": [
        "# Create WGAN object and compile model\n",
        "\n",
        "wgan = WGAN(\n",
        "    discriminator = discriminator,\n",
        "    generator = generator,\n",
        "    latent_dim=noise_dim,\n",
        "    disc_extra_steps=5,\n",
        ")\n",
        "\n",
        "wgan.compile(\n",
        "    d_optimizer=discriminator_optimizer,\n",
        "    g_optimizer=generator_optimizer,\n",
        "    g_loss_fn=generator_loss,\n",
        "    d_loss_fn=discriminator_loss\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1N4lLfC8Lc-"
      },
      "source": [
        "# Training model\n",
        "varGan = wgan.fit(training_data, batch_size=BATCH_SIZE, epochs=epochs, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyEyDLCaCeFq"
      },
      "source": [
        "# Plotting the loss functions during training for discriminator and generator\n",
        "plt.plot(varGan.history['d_loss'], label = \"Discriminator loss\")\n",
        "plt.plot(varGan.history['g_loss'], label = \"Generator loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EfQQT9dg5Oh"
      },
      "source": [
        "# Saving generator and discriminator models\n",
        "generator.save('generator_pretrained.h5')\n",
        "discriminator.save('discriminator_pretrained.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEOVYtoFmUuS"
      },
      "source": [
        "# Estimating VaR with the Trained Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVYSLOX6ekKB"
      },
      "source": [
        "# Simulation to estimate 95% VaR\n",
        "def varGan_sim(nbr_sim = 1000, k=5, generator = generator ):\n",
        "    p_w = 1/k\n",
        "    portfolio_weights = p_w * np.ones((k,1))\n",
        "    VaR_list = []\n",
        "    for i in range(nbr_sim):\n",
        "      noise=np.random.normal(size=(noise_dim))\n",
        "      noise=noise.reshape(1,noise_dim)\n",
        "      sim_vals = generator([noise])\n",
        "      # Transform to original scaling \n",
        "      sim_org = sim_vals*std_tot + mean_tot\n",
        "      sim_org = sim_org.numpy()\n",
        "      sim_org = sim_org.reshape(k,1)\n",
        "      # Simulated return for portfolio \n",
        "      p_return =np.matmul(portfolio_weights.transpose(), sim_org)\n",
        "      VaR_list.append(p_return)\n",
        "    # Estimate VaR from simulated returns in VaR_list       \n",
        "    VaR = np.percentile(VaR_list, 5)    \n",
        "    return VaR     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve8_ps0he2PX"
      },
      "source": [
        "valueatrisk = varGan_sim()\n",
        "valueatrisk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRdUvMXLeqS7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}